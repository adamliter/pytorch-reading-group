{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Readings\n",
    "\n",
    "The main readings necessary for this session are:\n",
    "- the [PyTorch 60 Minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- for understanding **word embeddings** and **continuous bag of words (CBoW) training**, I recommend Jurafsky's [Speech and Language Processing (SLP3) Ch. 16](https://web.stanford.edu/~jurafsky/slp3/16.pdf).\n",
    "\n",
    "This notebook assumes you're already familiar with such concepts as feedforward neural networks, stochastic gradient descent. If you're unfamiliar with (or need a refresher on) any of those topics, I recommend the following resources:\n",
    "- firstly, if you are unfamiliar with **feedforward neural networks** or **backpropagation**, I recommend no resource more than Michael Neilsen's [Neural Networks and Deep Learning Ch. 1 & 2](http://neuralnetworksanddeeplearning.com/chap1.html). It's a long read (they're 2 chapters of a book), but it is highly approachable, and very thorough. If you only need a refresher, or would just appreciate something shorter, you can also use [...].\n",
    "- if you are familiar with **logistic regression**, but unfamiliar with how a logistic regression classifier is trained using gradient descent, I highly recommend the [UFLDL Tutorial on Logistic Regression (and Softmax Regression)](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/). It goes into a lot of depth about both *what* logistic regression is, as well as how to derive the gradient updates. I strongly recommend going through the derivations for the gradient updates, even though we'll be using PyTorch's autograd.\n",
    "\n",
    "Once you have completed the readings you feel are necessary, come back and give this notebook a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Logistic Regression\n",
    "\n",
    "- the first task will be a logistic regression classifier for sentiment analysis\n",
    "- we'll use scikit-learn's tf-idf vectorizer to create the feature vectors\n",
    "- once we have the feature vectors, we'll need to convert them to torch variables\n",
    "\n",
    "$$h_{\\theta}(x) = \\frac{1}{1 + exp(-\\theta^T \\cdot x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\" Your code goes here. \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Off-the-Shelf Embeddings\n",
    "\n",
    "- SpaCy GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CBoW Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBoW(nn.Module):\n",
    "    \"\"\"Your code goes here.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py36)",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
